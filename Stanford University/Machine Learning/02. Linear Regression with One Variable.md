# [Machine Learning] (http://ml-class.com)

=================================

## Linear Regression with One Variable

Linear regression with one variable is `h(x)` (where `h` stands for **hypothesis**) where the function represents a linear slope and is also refereed as **Univariate linear regression**


### Cost Function

**Hypothesis**

```
h_theta(x^i) = theta_0 + theta_1*x^i        (2a)
```

We need to come up with the best approximation to fit the dataset. 

Idea: Choose `theta_0` , `theta_1` such that `h_theta(x)` is close to y for the training examples `(x,y)`. 

**Minimize `theta_0` and `theta_1`**

```
main = sum_(i=1)^m [ h_theta(x^i) - y^i ]^2  
                     |________|
                         |
                        (2a)    
```
```
min = 1/(2m) * main          (2b)
```
```
J(theta_0, theta_1)
|______________|
       |
  cost function (k)
```


Where:

-  `m` : The number of training examples
-  `k` : Also referred as squared error function
-  'sum': Summation


### Intuition 

**We have:**

- Hypothesis: `(2a)`
- Parameters :   `theta_0` , `theta_1`
- Cost Function : `(k)`
   + Where `k = (2b)`
	
**Simplified**

- Hypothesis : `h_theta(x) = theta_1*x`
 	+ `theta_0 = 0`
- Cost Function : `1/(2m) * sum_(i=1)^m [ h_theta(x^i) - y^i ]^2`
    + Where `h_theta(x^i) = theta_1*x^i`
	

**h_theta(x)**

For fixed `theta_1`, this is a function of `x`

```
theta_1 = 1
   
   J(theta_1) = 1/(2m) sum_(i=1)^m [ h_theta(x^i) - y^i ]^2 
              = 1/(2m) sum_(i=1)^m [ theta_1*x^i - y^i ]^2 
              = 1/(2m) (0^2 + 0^2 + 0^2)
              = 0   
```

**J(theta_1)**

Function of parameter `theta_1`

```
theta_1 = 0.5

   J(theta_1) = 1/(2m) sum_(i=1)^m [ h_theta(x^i) - y^i ]^2 
   J(0.5)     = 1/(2m) sum_(i=1)^m [ theta_1*x^i - y^i ]^2 
              = 1/(2m) ( (0.5-1)^2 + (1-2)^2 + (1.5-3)^2)
              = 1/(2*3) (3.5)   
              = 3.5/6
              = 0.58

theta_1 = 0

    J(0) = 1/2m(1^2+2^2+3^2) 
         = 1/2(3) * 14 = 2.3
	
theta_1 = -0.5

    J(-0.5)   = ~5.25
	
```

*The cost function is the sum of squares of all the x and y values*

Taking the following values results in a parabolic curve in the `J(theta_1)` graph.
The value that minimized `theta_1` is when `theta_1 = 1`

### Example 2

```
J(theta_0, theta_1)   is a 3D parabolic contour plot/surface
```


## Gradient Descent

**Assume**

- Have some function `J(theta_0, theta_1)`
- Want to minimize `J(theta_0, theta_1)`

**Algorithm Outline**

- Start with some `theta_0`, `theta_1`
- Keep changing `theta_0`, `theta_1` to reduce `J(theta_0, theta_1)` until we hopefully end up at a local minimum

**Gradient Descent Algorithm**

```
theta_j := theta_j - alpha * dye/(dye theta_j) * J(theta_0, theta_1=i)    (for j=0 and i=1)     (2c)
        ^              ^                                                         ^
        |              |                                                         |
  	assignment   learning rate                                        simultaneously update them
```


- Learning rate controls how big the step we take in order to converge

**Simultaneous Update**

**Correct**

```
temp0  := (2c)  [where denominator has 'dye theta_0']
temp1  := (2c)  [where denominator has 'dye theta_1']

theta_0 := temp0
theta_1 := temp1
```

**Incorrect**

```
temp0   := (2c)  [where denominator has 'dye theta_0' ]
theta_0 := temp0

temp1   := (2c)  [where denominator has 'dye theta_1' ]
theta_1 := temp1
```

The difference between them is that second equation in RHS uses the updated value of `theta_0` since the original value gets assigned.

### Intuition

If `alpha` is too small, it's going to increment or decrement by really small steps in order to reach the minimum. If `alpha` is too large, the gradient descent can overshoot the minimum and may fail to converge or diverge.

If `theta_1` is already at local minumum, then the slope is `0` then the derivative is `0` and `theta_1 stays theta_1`

Even if the learning rate `alpha` is fixed, as we approach the local minimum, gradient descent will automatically take smaller steps and you end up at the local minimum.


## The Linear Regression Algorithm


**Given `(2b)` and `(2c)`**

```
dye/(dye theta_j) * J(theta_0, theta_1=i)  = dye/(dye theta_j) * 1/(2m) sum_(i=1)^m [ h_theta(x^i) - y^i ]^2 
                                           = dye/(dye theta_j) * 1/(2m) sum_(i=1)^m [ theta_0 + theta_1*x^i - y^i ]^2
															
j = 0 : partial derivative = 1/m sum_(i=1)^m [h_theta(x^i) - u^i]
j = 1 : partial derivative = 1/m sum_(i=1)^m [h_theta(x^i) - y^i] * x^i

```

**Linear Regression**

```
theta_0 := theta_0 - alpha * 1/m sum_(i=1)^m [h_theta(x^i) - u^i]           
theta_1 := theta_0 - alpha * 1/m sum_(i=1)^m [h_theta(x^i) - u^i] * x^i  
```


These two are updated  **simultaneously** and this always results in a convex function.
